# Comparative Analysis of NiN and Multiâ€‘Branch Networks

This project presents a comparative study of **Networkâ€‘inâ€‘Network (NiN)** and a **multiâ€‘branch GoogLeNetâ€‘style architecture** for multiclass image classification and transfer learning. It was developed as a case study using the **Signâ€‘MNIST** dataset and extended to **MNIST** for transfer learning.

---

## ğŸ“Š Project Description

The goal is to evaluate two different deep neural architectures in terms of accuracy, convergence, parameter efficiency, and transferability.

### Datasets

* **Signâ€‘MNIST** â€“ Images of American Sign Language hand gestures (letters Aâ€“Y except J).
* **MNIST** â€“ Handwritten digit dataset used for transfer learning.

### Architectures

* **Networkâ€‘inâ€‘Network (NiN)** â€“ Uses 1Ã—1 convolutions (miniâ€‘MLPs) within convolutional layers to enhance nonlinear feature learning.
* **Multiâ€‘Branch Network (GoogLeNetâ€‘style)** â€“ Employs parallel convolutional branches (1Ã—1, 3Ã—3, 5Ã—5, pooling) to capture multiâ€‘scale features.

### Tasks

1. Train and evaluate NiN and GoogLeNet on **Signâ€‘MNIST**.
2. Apply **transfer learning** by adapting both models to **MNIST**.
3. Compare performance using accuracy, precision, recall, F1â€‘score, and resource requirements.

---

## ğŸ› ï¸ Requirements

* Python â‰¥ 3.9
* TensorFlow / Keras
* NumPy
* Matplotlib
* scikitâ€‘learn
* Jupyter

Install with:

```bash
pip install -r requirements.txt
```

---

## ğŸš€ Usage

Install the dependencies to run the project:

```bash
pip install -r requirements.txt
```

Then run `case_study_1.ipynb` in Jupyter or Google Colab.

---

## ğŸ“ˆ Results

* On **Signâ€‘MNIST**:

  * NiN achieved â‰ˆ **98.37% accuracy** with \~180k parameters.
  * GoogLeNet achieved â‰ˆ **93.61% accuracy** with \~37k parameters.
* On **MNIST (Transfer Learning)**:

  * NiN achieved â‰ˆ **82.88% accuracy**.
  * GoogLeNet achieved â‰ˆ **92.64% accuracy**.

**Key Insight:** NiN excels on the original dataset with higher accuracy, but GoogLeNet generalizes better in transfer learning with fewer parameters.

---

## ğŸ“‚ Files

* `NeuralNetworksComparativeAnalysis.ipynb` â€“ Main notebook with training and evaluation
* `requirements.txt` â€“ Dependencies
* `README.md` â€“ This file

