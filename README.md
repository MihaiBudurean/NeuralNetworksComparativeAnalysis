# Comparative Analysis of NiN and Multiâ€‘Branch Networks

This project presents a comparative study of **Networkâ€‘inâ€‘Network (NiN)** and a **multiâ€‘branch GoogLeNetâ€‘style architecture** for multiclass image classification and transfer learning. It was developed as a case study using the **Signâ€‘MNIST** dataset and extended to **MNIST** for transfer learning.

---

## ðŸ“Š Project Description

The goal is to evaluate two different deep neural architectures in terms of accuracy, convergence, parameter efficiency, and transferability.

### Datasets

* **Signâ€‘MNIST**
* **MNIST**

### Architectures

* **Networkâ€‘inâ€‘Network (NiN)** â€“ Uses 1Ã—1 convolutions (miniâ€‘MLPs) within convolutional layers to enhance nonlinear feature learning.
* **Multiâ€‘Branch Network (GoogLeNetâ€‘style)** â€“ Employs parallel convolutional branches (1Ã—1, 3Ã—3, 5Ã—5, pooling) to capture multiâ€‘scale features.

### Tasks

1. Train and evaluate NiN and GoogLeNet on **Signâ€‘MNIST**.
2. Apply **transfer learning** by adapting both models to **MNIST**.
3. Compare performance using accuracy, precision, recall, F1â€‘score, and resource requirements.

---

## ðŸ›  Requirements

Install the dependencies to run the project:

```bash
pip install -r requirements.txt
```

---

## ðŸ“ˆ Results

* On **Signâ€‘MNIST**:

  * NiN achieved â‰ˆ **98.37% accuracy** with \~180k parameters.
  * GoogLeNet achieved â‰ˆ **93.61% accuracy** with \~37k parameters.
* On **MNIST (Transfer Learning)**:

  * NiN achieved â‰ˆ **82.88% accuracy**.
  * GoogLeNet achieved â‰ˆ **92.64% accuracy**.

**Key Insight:** NiN excels on the original dataset with higher accuracy, but GoogLeNet generalizes better in transfer learning with fewer parameters.

